{"cells":[{"cell_type":"markdown","source":["###### *Notebook developed by John Uzoma*"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"9b316591-ba9c-499b-a4c6-0e708151063a"},{"cell_type":"code","source":["# initialize empty parameter for file path to be passed in from data pipeline\n","PARAM_file_path = \"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"049c1c71-1dae-42aa-86ae-ef7b1e7bf94c","statement_id":3,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-03T13:27:12.8406369Z","session_start_time":"2024-04-03T13:27:13.1066954Z","execution_start_time":"2024-04-03T13:27:22.7342959Z","execution_finish_time":"2024-04-03T13:27:24.7283065Z","parent_msg_id":"41a5ee48-1cc4-422c-a5d6-3a8f1b6dc058"},"text/plain":"StatementMeta(, 049c1c71-1dae-42aa-86ae-ef7b1e7bf94c, 3, Finished, Available)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"d533cf5c-4d13-405c-ad45-d0d48f8afa48"},{"cell_type":"markdown","source":["## Load from JSON to pySpark dataframe\n","###### Since I intend to include this notebook as part of a data pipeline that will run automatically on a schedule, I will create a function to retrieve the file path."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"c3d777c8-1fb9-4f5d-85dc-34ca713c2ed7"},{"cell_type":"code","source":["from datetime import datetime\n","\n","def read_in_todays_data():\n","    '''\n","    Function that dynamically retrieves the filepath\n","    Returns a dataframe of today's data\n","    '''\n","    try:\n","        # Load JSON data into a dataframe\n","        df = spark.read.json(PARAM_file_path)\n","        return df\n","    except Exception as err:\n","        print(err)\n","\n","df = read_in_todays_data()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":"049c1c71-1dae-42aa-86ae-ef7b1e7bf94c","statement_id":4,"state":"finished","livy_statement_state":"available","queued_time":"2024-04-03T13:27:12.845509Z","session_start_time":null,"execution_start_time":"2024-04-03T13:27:25.1116171Z","execution_finish_time":"2024-04-03T13:27:25.3651766Z","parent_msg_id":"d471e653-e880-43af-88c5-0efbe60aacf7"},"text/plain":"StatementMeta(, 049c1c71-1dae-42aa-86ae-ef7b1e7bf94c, 4, Finished, Available)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Can not create a Path from an empty string\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"078b1b46-8ce6-46fb-ab9e-52ecaa99b799"},{"cell_type":"markdown","source":["## Data cleaning (with code refactoring using functions)"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"bd54f79b-e614-499d-9c5a-e923d25743ed"},{"cell_type":"code","source":["from pyspark.sql.functions import col, to_timestamp, round, lit\n","\n","# function to convert date column from unix to datetime\n","def convert_unix_to_datetime(unix_datetime_col):\n","    return to_timestamp(unix_datetime_col)\n","\n","# function to convert temperature column from kelvin to celsius and fahrenheit, rounded to 2 decimal places\n","def temperature_conversion(kelvin_col, to_unit):\n","    if to_unit == 'celsius':\n","        return round(kelvin_col - 273.15, 2)\n","    elif to_unit == 'fah':\n","        return round((col(\"main.temp\") * 9/5) - 459.67, 2)\n","\n","# use df.select to flatten the structure of our nested JSON\n","flattened_df = df.select(\n","    convert_unix_to_datetime(col(\"dt\")).alias(\"datetime\"),\n","    col(\"main.temp\").alias(\"temperature_kelvin\"),\n","    temperature_conversion(col(\"main.temp\"), to_unit=\"celsius\").alias(\"temperature_celsius\"),\n","    temperature_conversion(col(\"main.temp\"), to_unit=\"fah\").alias(\"temperature_fahrenheit\")\n",")\n","\n","# create a new column 'Type' with value 'Historic'\n","flattened_df = flattened_df.withColumn(\"Type\", lit(\"Historic\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"5dd08142-654f-42a5-ab44-3122fdd343c3"},{"cell_type":"markdown","source":["## Load the dataframe into a Lakehouse table\n","###### I used the append method to load the row of data into my Lakehouse table, while preserving existing data."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"645da1fa-9d70-46fa-8956-2b98291b85af"},{"cell_type":"code","source":["flattened_df.write.format(\"delta\").mode(\"append\").save(\"Tables/historic_weather_data\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"8b627e17-2bd8-4bcb-a5e3-2dfba246205f"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"07789afb-4a11-4254-8649-f6007d9f443e","known_lakehouses":[{"id":"07789afb-4a11-4254-8649-f6007d9f443e"}],"default_lakehouse_name":"weather_lakehouse","default_lakehouse_workspace_id":"1a2ce695-a6f3-4878-b98a-2bdd38702570"}}},"nbformat":4,"nbformat_minor":5}