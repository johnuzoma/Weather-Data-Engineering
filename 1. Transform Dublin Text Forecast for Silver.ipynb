{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd97cfe-2e70-4620-9d81-f770d973b560",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <u>Notebook by John Uzoma</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4d174-83da-4053-aaa7-9d672e61c932",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load JSON file to a flattened pySpark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d506bda-9a13-42b8-a1e4-11c50dd207e5",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Create a function to flatten the dataframe and select required columns\n",
    "def flatten_df(df_name):\n",
    "   return df_name.select(\n",
    "            expr(\"forecasts.regions[0][1].issued\").alias(\"Issued\"),\n",
    "            expr(\"forecasts.regions[0][2].today\").alias(\"Today\"),\n",
    "            expr(\"forecasts.regions[0][3].tonight\").alias(\"Tonight\"),\n",
    "            expr(\"forecasts.regions[0][4].tomorrow\").alias(\"Tomorrow\"),\n",
    "            expr(\"forecasts.regions[0][5].outlook\").alias(\"Outlook\")\n",
    "        )\n",
    "\n",
    "# Define the path to the JSON file\n",
    "json_file_path = \"Files/Bronze/DublinTextForecast.json\"\n",
    "\n",
    "# Load JSON data into a dataframe\n",
    "df = flatten_df(spark.read.json(json_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fe903-7524-4aeb-a05e-b2fcc4bbd590",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Replace nulls and blanks in Outlook column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0bcd7-fad6-4cdd-ab4f-31931772eff8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col\n",
    " \n",
    "# Replace null or empty values in Outlook column with \"Unknown\"\n",
    "df = df.withColumn(\"Outlook\", when((col(\"Outlook\").isNull() | (col(\"Outlook\")==\"\")),lit(\"Unknown\")).otherwise(col(\"Outlook\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d319c-321d-4b99-95c2-c1d7b6344959",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### More transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718b6e2-31ee-4a22-8c36-7b0febd3cb97",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring, to_date, from_utc_timestamp\n",
    "\n",
    "# Convert Issued column to Irish timestamp\n",
    "df = df.withColumn(\"Issued\", from_utc_timestamp(col(\"Issued\"), \"Europe/Dublin\"))\n",
    "\n",
    "# Add 'Issue_Time' column and convert Issued column to date type\n",
    "df = df.withColumn(\"TextForecastIssueTime\", substring(col(\"Issued\"), 12, 19)) \\\n",
    "       .withColumn(\"Issued\", to_date(col(\"Issued\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ac2ad0-5e48-4458-85b3-0c9e846533cf",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define schema for silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f81379-ab53-452e-b359-c3f78ce96bcc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, StringType, DateType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"lakehouse.dublintextforecast_silver\") \\\n",
    "    .addColumn(\"Issued\", DateType()) \\\n",
    "    .addColumn(\"TextForecastIssueTime\", StringType()) \\\n",
    "    .addColumn(\"Today\", StringType()) \\\n",
    "    .addColumn(\"Tonight\", StringType()) \\\n",
    "    .addColumn(\"Tomorrow\", StringType()) \\\n",
    "    .addColumn(\"Outlook\", StringType()) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df12f0-14df-4f5e-a90e-1eb0c5994a70",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Optimize Delta table writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fddfe0-86da-4bbf-bfc1-0b4db0dec9cc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " # Enable V-Order\n",
    " spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "    \n",
    " # Enable automatic Delta optimized write\n",
    " spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a03da5-25f8-4cd8-9543-f62c94b7d61d",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Write dataframe to the silver table (overwrite operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b6a7b-bdb5-4b1e-8695-730672647990",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"Tables/dublintextforecast_silver\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "06a9b01e-67c7-4d49-b134-528745660f6b",
    "default_lakehouse_name": "lakehouse",
    "default_lakehouse_workspace_id": "6c2dfd82-79ca-43af-b447-84d78a797dd3"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
