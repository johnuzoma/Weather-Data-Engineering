{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cc036f4-517d-482b-a953-7944dd79b46a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <u>Notebook by John Uzoma</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c1630-5f87-4bcf-ae9a-187e48a2678e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define schema for dataframe and load json file into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06c627-5a10-42de-8cd4-dcd66dc57b9b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "    \n",
    " # Create the schema for the table\n",
    "df_schema = StructType([\n",
    "    StructField(\"level\", StringType()),\n",
    "    StructField(\"severity\", StringType()),\n",
    "    StructField(\"certainty\", StringType()),\n",
    "    StructField(\"issued\", TimestampType()),\n",
    "    StructField(\"updated\", TimestampType()),\n",
    "    StructField(\"onset\", TimestampType()),\n",
    "    StructField(\"expiry\", TimestampType()),\n",
    "    StructField(\"headline\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"status\", StringType())\n",
    "])\n",
    "\n",
    "# Define the path to the JSON file\n",
    "json_file_path = \"Files/Bronze/DublinWeatherWarning.json\"\n",
    "\n",
    "# Load JSON data into dataframe\n",
    "df = spark.read.schema(df_schema).json(json_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5410c6-cf80-43fb-a228-e6ec508745c5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068055ae-6341-463a-9350-d61a39158f29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = df \\\n",
    "    .withColumnRenamed(\"level\", \"Level\") \\\n",
    "    .withColumnRenamed(\"severity\", \"Severity\") \\\n",
    "    .withColumnRenamed(\"certainty\", \"Certainty\") \\\n",
    "    .withColumnRenamed(\"issued\", \"Issued\") \\\n",
    "    .withColumnRenamed(\"updated\", \"Updated\") \\\n",
    "    .withColumnRenamed(\"onset\", \"Onset\") \\\n",
    "    .withColumnRenamed(\"expiry\", \"Expiry\") \\\n",
    "    .withColumnRenamed(\"headline\", \"Headline\") \\\n",
    "    .withColumnRenamed(\"description\", \"Description\") \\\n",
    "    .withColumnRenamed(\"status\", \"Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d82e53-d386-40ba-a417-79b96ff672e0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### More transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cbb74-0f16-4fd9-8252-82302e6a3004",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring, to_date, from_utc_timestamp\n",
    "\n",
    "timestamp_cols = [\"Issued\", \"Updated\", \"Onset\", \"Expiry\"]\n",
    "\n",
    "for col_name in timestamp_cols:\n",
    "       # Convert the columns to Irish timestamp\n",
    "       df = df.withColumn(col_name, from_utc_timestamp(col(col_name), \"Europe/Dublin\"))\n",
    "\n",
    "# Add 'Issue_Time' column and convert Issued column to date type\n",
    "df = df.withColumn(\"WarningIssueTime\", substring(col(\"Issued\"), 12, 19)) \\\n",
    "       .withColumn(\"Issued\", to_date(col(\"Issued\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb7609-92d9-4345-a4be-5f9ea32d2c55",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define schema for silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0f339-4026-4a4a-9d88-e400ee40136a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"lakehouse.dublinweatherwarning_silver\") \\\n",
    "    .addColumn(\"Level\", StringType()) \\\n",
    "    .addColumn(\"Severity\", StringType()) \\\n",
    "    .addColumn(\"Certainty\", StringType()) \\\n",
    "    .addColumn(\"Issued\", DateType()) \\\n",
    "    .addColumn(\"WarningIssueTime\", StringType()) \\\n",
    "    .addColumn(\"Updated\", TimestampType()) \\\n",
    "    .addColumn(\"Onset\", TimestampType()) \\\n",
    "    .addColumn(\"Expiry\", TimestampType()) \\\n",
    "    .addColumn(\"Headline\", StringType()) \\\n",
    "    .addColumn(\"Description\", StringType()) \\\n",
    "    .addColumn(\"Status\", StringType()) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ed09a-ccf7-422e-b1b0-cac2917f2714",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Optimize delta table writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a8d46-f931-4273-aa83-1a1d6247eab0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " # Enable V-Order\n",
    " spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "    \n",
    " # Enable automatic Delta optimized write\n",
    " spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729bb9ef-6db1-4da3-b680-a7e81a26fec1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Write the dataframe to silver table (append operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b117f74-c275-4741-93a7-ba111ddf7a87",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"append\").save(\"Tables/dublinweatherwarning_silver\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "06a9b01e-67c7-4d49-b134-528745660f6b",
    "default_lakehouse_name": "lakehouse",
    "default_lakehouse_workspace_id": "6c2dfd82-79ca-43af-b447-84d78a797dd3"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
