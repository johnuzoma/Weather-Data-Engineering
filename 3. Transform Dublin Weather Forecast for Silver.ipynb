{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cfc9085-74cc-4c25-a83e-32ca2e16c40b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## <u>Notebook by John Uzoma</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9985df05-1972-4f83-8543-d963eccc80dd",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Read the XML text file as a string, then convert to bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3cce9-88d5-47ac-a9f8-d43812fbf7fc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define the input file path\n",
    "input_file_path = \"Files/Bronze/DublinWeatherForecast.txt\"\n",
    "\n",
    "# Read the text file using SparkContext and collect the content as a list of strings\n",
    "file_lines = spark.sparkContext.textFile(input_file_path).collect()\n",
    "\n",
    "# Convert the list of lines to a single string with newline characters\n",
    "file_content = \"\\n\".join(file_lines)\n",
    "\n",
    "# Convert the string to bytes using UTF-8 encoding\n",
    "file_bytes = file_content.encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055545e1-a2aa-479a-bdbb-0749c8ed7d2e",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Parse XML data and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563acb0-41f8-4f3d-9583-fb54ae970570",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Parse XML data\n",
    "root = ET.fromstring(file_bytes)\n",
    "\n",
    "# Initialize empty list\n",
    "data = []\n",
    "\n",
    "# Find all <time> elements using a loop and capture relevant values\n",
    "for time in root.findall(\".//time\"):\n",
    "    dateTime = time.get(\"from\") if time is not None else None\n",
    "    location = time.find(\"./location\")\n",
    "    temperature = location.find(\"./temperature\").get(\"value\") if location.find(\"./temperature\") is not None else None\n",
    "    windDirection = location.find(\"./windDirection\").get(\"name\") if location.find(\"./windDirection\") is not None else None\n",
    "    windSpeed = location.find(\"./windSpeed\").get(\"mps\") if location.find(\"./windSpeed\") is not None else None\n",
    "    windGust = location.find(\"./windGust\").get(\"mps\") if location.find(\"./windGust\") is not None else None\n",
    "    globalRadiation = location.find(\"./globalRadiation\").get(\"value\") if location.find(\"./globalRadiation\") is not None else None\n",
    "    humidity = location.find(\"./humidity\").get(\"value\") if location.find(\"./humidity\") is not None else None\n",
    "    pressure = location.find(\"./pressure\").get(\"value\") if location.find(\"./pressure\") is not None else None\n",
    "    cloudiness = location.find(\"./cloudiness\").get(\"percent\") if location.find(\"./cloudiness\") is not None else None\n",
    "    dewpointTemperature = location.find(\"./dewpointTemperature\").get(\"value\") if location.find(\"./dewpointTemperature\") is not None else None\n",
    "\n",
    "    # Append values to list\n",
    "    data.append((\n",
    "        dateTime, \n",
    "        temperature, \n",
    "        windDirection, \n",
    "        windSpeed, \n",
    "        windGust, \n",
    "        globalRadiation, \n",
    "        humidity, \n",
    "        pressure,\n",
    "        cloudiness,\n",
    "        dewpointTemperature\n",
    "    ))\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    \"DateTime\",\n",
    "    \"Temperature_celsius\",\n",
    "    \"WindDirection\",\n",
    "    \"WindSpeed_mps\",\n",
    "    \"WindGust_mps\",\n",
    "    \"GlobalRadiation_wpsqm\",\n",
    "    \"Humidity_percent\",\n",
    "    \"Pressure_hPa\",\n",
    "    \"Cloudiness_percent\",\n",
    "    \"DewpointTemperature_celsius\"\n",
    "]\n",
    "\n",
    "# Create the DataFrame with column names, dropping any null records\n",
    "df1 = spark.createDataFrame(data, columns).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058cd80f-fb53-4193-a2ab-c55fe1c0aa8a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Initialize empty list\n",
    "data = []\n",
    "\n",
    "# Find all <time> elements using a loop and capture relevant values\n",
    "for time in root.findall(\".//time\"):\n",
    "    forecastFrom = time.get(\"from\") if time is not None else None\n",
    "    forecastTo = time.get(\"to\") if time is not None else None\n",
    "    location = time.find(\"./location\")\n",
    "    precipitation = location.find(\"./precipitation\").get(\"value\") if location.find(\"./precipitation\") is not None else None\n",
    "    symbol = location.find(\"./symbol\").get(\"id\") if location.find(\"./symbol\") is not None else None\n",
    "\n",
    "    # Append values to list\n",
    "    data.append((\n",
    "        forecastFrom,\n",
    "        forecastTo,\n",
    "        precipitation,\n",
    "        symbol\n",
    "    ))\n",
    "\n",
    "# Define column names\n",
    "columns = [\n",
    "    \"ForecastFrom\",\n",
    "    \"ForecastTo\",\n",
    "    \"Precipitation_mm\",\n",
    "    \"WeatherType\"\n",
    "]\n",
    "\n",
    "# Create the DataFrame with column names, dropping any null records\n",
    "df2 = spark.createDataFrame(data, columns).na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919f8d9-62d1-45df-9618-13504e0b33e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Perform the inner join between df1 and df2, keeping all columns from both dataframes\n",
    "joined_df = df1.join(df2, df1[\"DateTime\"] == df2[\"ForecastTo\"], \"inner\").select(df1[\"*\"], df2[\"*\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c3ec1-1090-4e6c-bda3-a44cd8ab4aa3",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Convert datatype(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ba28e-5d39-45bf-8622-f247fc52b05b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, from_utc_timestamp\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Store to-be timestamp columns in a list\n",
    "timestamp_cols = [\"DateTime\", \"ForecastFrom\", \"ForecastTo\"]\n",
    "\n",
    "# Store to-be float columns in a list\n",
    "float_cols = [\n",
    "    \"Temperature_celsius\", \n",
    "    \"WindSpeed_mps\",\n",
    "    \"WindGust_mps\",\n",
    "    \"GlobalRadiation_wpsqm\",\n",
    "    \"Humidity_percent\",\n",
    "    \"Pressure_hPa\",\n",
    "    \"Cloudiness_percent\",\n",
    "    \"DewpointTemperature_celsius\",\n",
    "    \"Precipitation_mm\"\n",
    "]\n",
    "\n",
    "# Convert relevant columns to Irish timestamp\n",
    "for col_name in timestamp_cols:\n",
    "    joined_df = joined_df.withColumn(col_name, from_utc_timestamp(col(col_name), \"Europe/Dublin\"))\n",
    "\n",
    "# Convert relevant columns to float\n",
    "for col_name in float_cols:\n",
    "    joined_df = joined_df.withColumn(col_name, col(col_name).cast(FloatType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65bd32-3d9f-4856-95bc-76d877ae08aa",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Add new columns to store previous values of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d5c27-704e-405f-8577-c21a00ce5e35",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Define the window specification\n",
    "windowSpec = Window.orderBy(\"DateTime\")\n",
    "\n",
    "# Add previous temperature column\n",
    "joined_df = joined_df.withColumn(\"PreviousTemperature\", lag(col(\"Temperature_celsius\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousCloudiness\", lag(col(\"Cloudiness_percent\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousWindSpeed\", lag(col(\"WindSpeed_mps\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousWindGust\", lag(col(\"WindGust_mps\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousGlobalRadiation\", lag(col(\"GlobalRadiation_wpsqm\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousHumidity\", lag(col(\"Humidity_percent\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousPressure\", lag(col(\"Pressure_hPa\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousDewpointTemperature\", lag(col(\"DewpointTemperature_celsius\"), 1).over(windowSpec)) \\\n",
    "                .withColumn(\"PreviousPrecipitation\", lag(col(\"Precipitation_mm\"), 1).over(windowSpec)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb59bfa-9e82-46e7-8c34-d57117850be8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### More transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87110a63-8c4e-4307-99fb-e59385d815e2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring, to_date\n",
    "\n",
    "# Split DateTime (Add Time and Date columns)\n",
    "joined_df = joined_df.withColumn(\"Time\", substring(col(\"DateTime\"), 12, 19)) \\\n",
    "       .withColumn(\"Date\", to_date(col(\"DateTime\")))\n",
    "\n",
    "# Drop DateTime column\n",
    "joined_df = joined_df.drop(\"DateTime\")\n",
    "\n",
    "# Sort the dataframe in ascending order by DateTime\n",
    "joined_df = joined_df.orderBy(col(\"Date\").asc(), col(\"Time\").asc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8594e4c-fa5b-4fc2-9e7c-a04ffce6f5ba",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Define schema for silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cb7339-1981-44d4-83b2-c4a5810c1d20",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType, StringType, FloatType, DateType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"lakehouse.dublinweatherforecast_silver\") \\\n",
    "    .addColumn(\"Date\", DateType()) \\\n",
    "    .addColumn(\"Time\", StringType()) \\\n",
    "    .addColumn(\"Temperature_celsius\", FloatType()) \\\n",
    "    .addColumn(\"PreviousTemperature\", FloatType()) \\\n",
    "    .addColumn(\"WindDirection\", StringType()) \\\n",
    "    .addColumn(\"WindSpeed_mps\", FloatType()) \\\n",
    "    .addColumn(\"PreviousWindSpeed\", FloatType()) \\\n",
    "    .addColumn(\"WindGust_mps\", FloatType()) \\\n",
    "    .addColumn(\"PreviousWindGust\", FloatType()) \\\n",
    "    .addColumn(\"GlobalRadiation_wpsqm\", FloatType()) \\\n",
    "    .addColumn(\"PreviousGlobalRadiation\", FloatType()) \\\n",
    "    .addColumn(\"Humidity_percent\", FloatType()) \\\n",
    "    .addColumn(\"PreviousHumidity\", FloatType()) \\\n",
    "    .addColumn(\"Pressure_hPa\", FloatType()) \\\n",
    "    .addColumn(\"PreviousPressure\", FloatType()) \\\n",
    "    .addColumn(\"Cloudiness_percent\", FloatType()) \\\n",
    "    .addColumn(\"PreviousCloudiness\", FloatType()) \\\n",
    "    .addColumn(\"DewpointTemperature_celsius\", FloatType()) \\\n",
    "    .addColumn(\"PreviousDewpointTemperature\", FloatType()) \\\n",
    "    .addColumn(\"ForecastFrom\", TimestampType()) \\\n",
    "    .addColumn(\"ForecastTo\", TimestampType()) \\\n",
    "    .addColumn(\"Precipitation_mm\", FloatType()) \\\n",
    "    .addColumn(\"PreviousPrecipitation\", FloatType()) \\\n",
    "    .addColumn(\"WeatherType\", StringType()) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bc596-7e04-42ed-ae0b-22cbe3281c2f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Optimize delta table writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a83f2c-4dd8-46b0-8c00-476556be24b7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    " # Enable V-Order\n",
    " spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n",
    "    \n",
    " # Enable automatic Delta optimized write\n",
    " spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada6b7a-0c95-45f4-aebb-665ff7a1874f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Write the dataframe to silver table (upsert operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1b6c7-e56e-4098-af92-76a16fcdf8d5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Update existing records and insert new ones based on a condition defined by the columns: Date and Time\n",
    "    \n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/dublinweatherforecast_silver')    \n",
    "\n",
    "dfUpdates = joined_df\n",
    "    \n",
    "deltaTable.alias('silver') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'silver.Date = updates.Date and silver.Time = updates.Time'\n",
    "  ) \\\n",
    "   .whenMatchedUpdate(set =\n",
    "    {\n",
    "      \"Temperature_celsius\": \"updates.Temperature_celsius\",\n",
    "      \"PreviousTemperature\": \"updates.PreviousTemperature\",\n",
    "      \"WindDirection\": \"updates.WindDirection\",\n",
    "      \"WindSpeed_mps\": \"updates.WindSpeed_mps\",\n",
    "      \"PreviousWindSpeed\": \"updates.PreviousWindSpeed\",\n",
    "      \"WindGust_mps\": \"updates.WindGust_mps\",\n",
    "      \"PreviousWindGust\": \"updates.PreviousWindGust\",\n",
    "      \"GlobalRadiation_wpsqm\": \"updates.GlobalRadiation_wpsqm\",\n",
    "      \"PreviousGlobalRadiation\": \"updates.PreviousGlobalRadiation\",\n",
    "      \"Humidity_percent\": \"updates.Humidity_percent\",\n",
    "      \"PreviousHumidity\": \"updates.PreviousHumidity\",\n",
    "      \"Pressure_hPa\": \"updates.Pressure_hPa\",\n",
    "      \"PreviousPressure\": \"updates.PreviousPressure\",\n",
    "      \"Cloudiness_percent\": \"updates.Cloudiness_percent\",\n",
    "      \"PreviousCloudiness\": \"updates.PreviousCloudiness\",\n",
    "      \"DewpointTemperature_celsius\": \"updates.DewpointTemperature_celsius\",\n",
    "      \"PreviousDewpointTemperature\": \"updates.PreviousDewpointTemperature\",\n",
    "      \"ForecastFrom\": \"updates.ForecastFrom\",\n",
    "      \"ForecastTo\": \"updates.ForecastTo\",\n",
    "      \"Precipitation_mm\": \"updates.Precipitation_mm\",\n",
    "      \"PreviousPrecipitation\": \"updates.PreviousPrecipitation\",\n",
    "      \"WeatherType\": \"updates.WeatherType\"\n",
    "    }\n",
    "  ) \\\n",
    " .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"Date\": \"updates.Date\",\n",
    "      \"Time\": \"updates.Time\",\n",
    "      \"Temperature_celsius\": \"updates.Temperature_celsius\",\n",
    "      \"PreviousTemperature\": \"updates.PreviousTemperature\",\n",
    "      \"WindDirection\": \"updates.WindDirection\",\n",
    "      \"WindSpeed_mps\": \"updates.WindSpeed_mps\",\n",
    "      \"PreviousWindSpeed\": \"updates.PreviousWindSpeed\",\n",
    "      \"WindGust_mps\": \"updates.WindGust_mps\",\n",
    "      \"PreviousWindGust\": \"updates.PreviousWindGust\",\n",
    "      \"GlobalRadiation_wpsqm\": \"updates.GlobalRadiation_wpsqm\",\n",
    "      \"PreviousGlobalRadiation\": \"updates.PreviousGlobalRadiation\",\n",
    "      \"Humidity_percent\": \"updates.Humidity_percent\",\n",
    "      \"PreviousHumidity\": \"updates.PreviousHumidity\",\n",
    "      \"Pressure_hPa\": \"updates.Pressure_hPa\",\n",
    "      \"PreviousPressure\": \"updates.PreviousPressure\",\n",
    "      \"Cloudiness_percent\": \"updates.Cloudiness_percent\",\n",
    "      \"PreviousCloudiness\": \"updates.PreviousCloudiness\",\n",
    "      \"DewpointTemperature_celsius\": \"updates.DewpointTemperature_celsius\",\n",
    "      \"PreviousDewpointTemperature\": \"updates.PreviousDewpointTemperature\",\n",
    "      \"ForecastFrom\": \"updates.ForecastFrom\",\n",
    "      \"ForecastTo\": \"updates.ForecastTo\",\n",
    "      \"Precipitation_mm\": \"updates.Precipitation_mm\",\n",
    "      \"PreviousPrecipitation\": \"updates.PreviousPrecipitation\",\n",
    "      \"WeatherType\": \"updates.WeatherType\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "06a9b01e-67c7-4d49-b134-528745660f6b",
    "default_lakehouse_name": "lakehouse",
    "default_lakehouse_workspace_id": "6c2dfd82-79ca-43af-b447-84d78a797dd3"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
